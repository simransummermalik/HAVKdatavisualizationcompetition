{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "17his8VamnF-",
        "outputId": "32a246be-9007-409a-b3be-9933da39100a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n# HAVK Data Visualization Sprint  \n**Date:** Monday, November 3, 2025  \n**Time:** 5:30 PM – 9:00 PM  \n**Location:** Woodward 335, UNC Charlotte  \n**Hosted by:** HAVK  \n\n---\n\n## Purpose of This Notebook\nThis notebook is provided **for reference and documentation only**.  \nIt contains setup instructions, library notes, and code templates designed to help participants replicate or expand upon their sprint work after the event.  \nDuring the competition, all analysis, coding, and interpretation must be performed independently within the official work period.\n\nThe goal is to support post-event learning, reproducibility, and portfolio development—not to serve as a shortcut or solution file.\n\n---\n\n## Overview\nThe HAVK Data Visualization Sprint is a focused, research-oriented challenge centered on **AI, Cybersecurity, Data Science, and Engineering** applications of data analysis.  \nParticipants explore newly released datasets, extract meaningful insights, and communicate their results clearly and logically.\n\nThis event emphasizes practical reasoning and real analytical workflows used in modern research, security, and technical environments.\n\n---\n\n## Schedule\n\n| Time | Activity |\n|:--:|:--|\n| **5:15 PM** | **Dataset Publication.** Official datasets are made public. Participants may review dataset summaries but may not begin work until 5:30 PM. |\n| **5:30 – 8:00 PM** | **Work Period.** Teams analyze, visualize, and interpret assigned datasets using their chosen tools. HAVK mentors and officers will be available for technical questions. |\n| **8:00 – 9:00 PM** | **Presentations and Judging.** Teams present findings to the judging panel. Winners are announced immediately following the final presentation. |\n\n---\n\n## Dataset Assignment\n- Datasets will be **released at 5:15 PM** and assigned at **5:30 PM** when all teams are seated.  \n- Each team receives a **team number** at check-in. Selection order is determined by a random draw.  \n- Once selected, a dataset is locked for the duration of the sprint.  \n- Datasets are chosen to represent real-world contexts in **AI research, cybersecurity monitoring, and scientific or engineering analytics**.  \n- Accessing or analyzing datasets before 5:15 PM is not permitted.  \n\n---\n\n## Team Structure\n- Teams may consist of **one or two members**.  \n- Collaboration between separate teams is not permitted once assignments are made.  \n- Mentors may provide clarification and troubleshooting help but will not perform analysis or generate visualizations.\n\n---\n\n## Technical Environment and Libraries\nYou may use **any language, framework, or visualization tool** that supports your analysis.  \nThis includes but is not limited to:\n\n- **Python:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `plotly`, `scikit-learn`  \n- **R:** `tidyverse`, `ggplot2`, `dplyr`, `shiny`  \n- **JavaScript:** `D3.js`, `Chart.js`, `TensorFlow.js`  \n- **C++ / Java:** for algorithmic or systems-based approaches  \n- **Tableau, Power BI, Excel:** for data visualization and dashboard creation  \n\nParticipants are encouraged to experiment with techniques relevant to **machine learning, anomaly detection, cybersecurity telemetry, and statistical pattern analysis**, as long as they can explain their process and reasoning.\n\nThis reference notebook includes notes on commonly used libraries for participants who are newer to coding or visualization.\n\n---\n\n## Use of Generative AI and External Resources\n**Generative AI tools** such as ChatGPT, Gemini, Copilot, or Claude may be used under the following guidelines:\n\n**Allowed:**  \n- Syntax help, debugging suggestions, or library documentation  \n- Formatting assistance for code or visualization setup  \n- Writing markdown explanations or summaries after you have completed your analysis  \n\n**Not Allowed:**  \n- Generating or performing full analyses of the provided dataset  \n- Submitting AI-generated interpretations as original work  \n- Uploading or exposing any competition dataset to third-party AI platforms that retain data  \n\nAll visualizations, insights, and conclusions must be your own.  \nExternal datasets may not be merged or cross-referenced without mentor approval.\n\n---\n\n## Objective\nYour task is to analyze your dataset and extract meaningful, technically sound insights.  \nYou are expected to:\n1. Identify a relevant question, pattern, or anomaly.  \n2. Use data-driven reasoning to support your findings.  \n3. Communicate results clearly through concise visuals and logical explanation.\n\nProjects should reflect real-world problem-solving in areas such as **AI model behavior, cybersecurity threat detection, or scientific data exploration**.\n\n---\n\n## Presentations and Judging\nPresentations will take place between **8:00 PM and 9:00 PM**.  \nEach team will have approximately **3 minutes** to present their findings in any preferred format:\n- Live walkthrough of your notebook  \n- Short slide deck or dashboard demonstration  \n- Direct explanation of plots and analysis  \n\nJudging criteria:\n\n| Criterion | Description |\n|:--|:--|\n| **Insight** | Depth and originality of findings. Does the work reveal something meaningful about system behavior, trends, or risk? |\n| **Clarity** | Logical flow and quality of explanation. Can your reasoning be followed by a technical audience? |\n| **Visualization** | Readability, structure, and effectiveness of plots or figures in supporting your conclusion. |\n\nAwards will be presented for:\n- **Best Overall Project**  \n- **Best Visualization**  \n- **Most Insightful Finding**\n\n---\n\n## Expectations\n- Bring a reliable laptop and any necessary cables or tools.  \n- Save work regularly and maintain version control when possible.  \n- Label all figures, variables, and outputs for readability.  \n- Maintain professionalism, teamwork, and academic integrity.  \n- This event prioritizes **critical thinking, accuracy, and communication** over volume of code.\n\n---\n\n**Once your dataset has been assigned, proceed to Section 1 – Setup below.**  \nThis notebook exists to guide your workflow, clarify library use, and serve as a record of your analytical process for future reference.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(r\"\"\"\n",
        "# HAVK Data Visualization Sprint\n",
        "**Date:** Monday, November 3, 2025\n",
        "**Time:** 5:30 PM – 9:00 PM\n",
        "**Location:** Woodward 335, UNC Charlotte\n",
        "**Hosted by:** HAVK\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose of This Notebook\n",
        "This notebook is provided **for reference and documentation only**.\n",
        "It contains setup instructions, library notes, and code templates designed to help participants replicate or expand upon their sprint work after the event.\n",
        "During the competition, all analysis, coding, and interpretation must be performed independently within the official work period.\n",
        "\n",
        "The goal is to support post-event learning, reproducibility, and portfolio development—not to serve as a shortcut or solution file.\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "The HAVK Data Visualization Sprint is a focused, research-oriented challenge centered on **AI, Cybersecurity, Data Science, and Engineering** applications of data analysis.\n",
        "Participants explore newly released datasets, extract meaningful insights, and communicate their results clearly and logically.\n",
        "\n",
        "This event emphasizes practical reasoning and real analytical workflows used in modern research, security, and technical environments.\n",
        "\n",
        "---\n",
        "\n",
        "## Schedule\n",
        "\n",
        "| Time | Activity |\n",
        "|:--:|:--|\n",
        "| **5:15 PM** | **Dataset Publication.** Official datasets are made public. Participants may review dataset summaries but may not begin work until 5:30 PM. |\n",
        "| **5:30 – 8:00 PM** | **Work Period.** Teams analyze, visualize, and interpret assigned datasets using their chosen tools. HAVK mentors and officers will be available for technical questions. |\n",
        "| **8:00 – 9:00 PM** | **Presentations and Judging.** Teams present findings to the judging panel. Winners are announced immediately following the final presentation. |\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Assignment\n",
        "- Datasets will be **released at 5:15 PM** and assigned at **5:30 PM** when all teams are seated.\n",
        "- Each team receives a **team number** at check-in. Selection order is determined by a random draw.\n",
        "- Once selected, a dataset is locked for the duration of the sprint.\n",
        "- Datasets are chosen to represent real-world contexts in **AI research, cybersecurity monitoring, and scientific or engineering analytics**.\n",
        "- Accessing or analyzing datasets before 5:15 PM is not permitted.\n",
        "\n",
        "---\n",
        "\n",
        "## Team Structure\n",
        "- Teams may consist of **one or two members**.\n",
        "- Collaboration between separate teams is not permitted once assignments are made.\n",
        "- Mentors may provide clarification and troubleshooting help but will not perform analysis or generate visualizations.\n",
        "\n",
        "---\n",
        "\n",
        "## Technical Environment and Libraries\n",
        "You may use **any language, framework, or visualization tool** that supports your analysis.\n",
        "This includes but is not limited to:\n",
        "\n",
        "- **Python:** `pandas`, `numpy`, `matplotlib`, `seaborn`, `plotly`, `scikit-learn`\n",
        "- **R:** `tidyverse`, `ggplot2`, `dplyr`, `shiny`\n",
        "- **JavaScript:** `D3.js`, `Chart.js`, `TensorFlow.js`\n",
        "- **C++ / Java:** for algorithmic or systems-based approaches\n",
        "- **Tableau, Power BI, Excel:** for data visualization and dashboard creation\n",
        "\n",
        "Participants are encouraged to experiment with techniques relevant to **machine learning, anomaly detection, cybersecurity telemetry, and statistical pattern analysis**, as long as they can explain their process and reasoning.\n",
        "\n",
        "This reference notebook includes notes on commonly used libraries for participants who are newer to coding or visualization.\n",
        "\n",
        "---\n",
        "\n",
        "## Use of Generative AI and External Resources\n",
        "**Generative AI tools** such as ChatGPT, Gemini, Copilot, or Claude may be used under the following guidelines:\n",
        "\n",
        "**Allowed:**\n",
        "- Syntax help, debugging suggestions, or library documentation\n",
        "- Formatting assistance for code or visualization setup\n",
        "- Writing markdown explanations or summaries after you have completed your analysis\n",
        "\n",
        "**Not Allowed:**\n",
        "- Generating or performing full analyses of the provided dataset\n",
        "- Submitting AI-generated interpretations as original work\n",
        "- Uploading or exposing any competition dataset to third-party AI platforms that retain data\n",
        "\n",
        "All visualizations, insights, and conclusions must be your own.\n",
        "External datasets may not be merged or cross-referenced without mentor approval.\n",
        "\n",
        "---\n",
        "\n",
        "## Objective\n",
        "Your task is to analyze your dataset and extract meaningful, technically sound insights.\n",
        "You are expected to:\n",
        "1. Identify a relevant question, pattern, or anomaly.\n",
        "2. Use data-driven reasoning to support your findings.\n",
        "3. Communicate results clearly through concise visuals and logical explanation.\n",
        "\n",
        "Projects should reflect real-world problem-solving in areas such as **AI model behavior, cybersecurity threat detection, or scientific data exploration**.\n",
        "\n",
        "---\n",
        "\n",
        "## Presentations and Judging\n",
        "Presentations will take place between **8:00 PM and 9:00 PM**.\n",
        "Each team will have approximately **3 minutes** to present their findings in any preferred format:\n",
        "- Live walkthrough of your notebook\n",
        "- Short slide deck or dashboard demonstration\n",
        "- Direct explanation of plots and analysis\n",
        "\n",
        "Judging criteria:\n",
        "\n",
        "| Criterion | Description |\n",
        "|:--|:--|\n",
        "| **Insight** | Depth and originality of findings. Does the work reveal something meaningful about system behavior, trends, or risk? |\n",
        "| **Clarity** | Logical flow and quality of explanation. Can your reasoning be followed by a technical audience? |\n",
        "| **Visualization** | Readability, structure, and effectiveness of plots or figures in supporting your conclusion. |\n",
        "\n",
        "Awards will be presented for:\n",
        "- **Best Overall Project**\n",
        "- **Best Visualization**\n",
        "- **Most Insightful Finding**\n",
        "\n",
        "---\n",
        "\n",
        "## Expectations\n",
        "- Bring a reliable laptop and any necessary cables or tools.\n",
        "- Save work regularly and maintain version control when possible.\n",
        "- Label all figures, variables, and outputs for readability.\n",
        "- Maintain professionalism, teamwork, and academic integrity.\n",
        "- This event prioritizes **critical thinking, accuracy, and communication** over volume of code.\n",
        "\n",
        "---\n",
        "\n",
        "**Once your dataset has been assigned, proceed to Section 1 – Setup below.**\n",
        "This notebook exists to guide your workflow, clarify library use, and serve as a record of your analytical process for future reference.\n",
        "\"\"\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(r\"\"\"\n",
        "# Section 1 — How to Think Like an Analyst Here\n",
        "This section is a guide. You do **not** have to follow it exactly.\n",
        "If you already know what you're doing, skip ahead.\n",
        "\n",
        "The goal of this sprint is not \"make a pretty plot.\"\n",
        "The goal is \"find something real and explain why it matters.\"\n",
        "\n",
        "Below is the standard workflow people use in AI, cybersecurity, data science, and engineering.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 0. Understand the dataset\n",
        "Before you write any code, answer this in plain English:\n",
        "- What does one row represent?\n",
        "  Example: one app listing, one employee record, one purchase event, one security alert, one lift attempt.\n",
        "- What does each column approximately mean?\n",
        "  (timestamp, rating, department, severity, price, etc.)\n",
        "\n",
        "If you can't answer what a single row means, stop here and figure that out.\n",
        "If you can't say what the columns are measuring, you cannot defend any conclusion later.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1. Load the data\n",
        "Most teams will be handed a CSV file.\n",
        "\n",
        "A CSV is just a table saved as plain text:\n",
        "- Comma-separated values.\n",
        "- Each line is one row.\n",
        "- First line usually has column names.\n",
        "\n",
        "In Python, people usually load CSV into a pandas DataFrame (a table-like object you can filter, group, summarize).\n",
        "\n",
        "In Excel / Tableau / Power BI, you'd import the CSV as a sheet or data source.\n",
        "\n",
        "More details on loading a CSV are in a later cell.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2. Inspect it\n",
        "Do not jump straight to graphs.\n",
        "You have to see what you're working with.\n",
        "\n",
        "Questions to ask:\n",
        "- How many rows are there?\n",
        "- What are the column names?\n",
        "- Which columns are numbers? Which are categories? Which are timestamps?\n",
        "- Are there missing values?\n",
        "\n",
        "If you're using pandas, people commonly call:\n",
        "- `df.head()` → first 5 rows\n",
        "- `df.info()` → column names and data types\n",
        "- `df.describe()` → summary stats\n",
        "\n",
        "If you're not using Python, you still do the same thinking:\n",
        "scroll the first ~20 rows and write down what's normal and what's weird.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3. Pick ONE question\n",
        "You are not trying to analyze everything at once.\n",
        "Pick a single question that matters and stick to it.\n",
        "\n",
        "Good questions:\n",
        "- Which category or group is actually dominating?\n",
        "- Did something spike (traffic, attrition, cost, severity) at a certain time?\n",
        "- Are two variables clearly related (for example: price vs rating, weight vs lift total)?\n",
        "- Is there a meaningful difference between groups (for example: department A vs department B)?\n",
        "\n",
        "Tie it to reality:\n",
        "- AI / model behavior\n",
        "- security / anomaly / incident\n",
        "- performance / reliability\n",
        "- business / behavior / usage\n",
        "\n",
        "If you can't state your question in one clean sentence, you are not ready to plot.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 4. Build evidence\n",
        "Now you create visual proof.\n",
        "\n",
        "This is where plotting comes in. Typical useful plots:\n",
        "- Bar chart comparing counts across categories\n",
        "- Line plot over time\n",
        "- Scatter plot showing the relationship between two numeric variables\n",
        "- Heatmap of correlations\n",
        "\n",
        "Each of these is shown in a later cell as a template.\n",
        "\n",
        "Important:\n",
        "- Your plot is not decoration.\n",
        "- Your plot is supposed to prove or disprove your question.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5. Write the finding in normal language\n",
        "Bad: \"Here is a bar chart.\"\n",
        "Good: \"Apps in the 'Tools' category have way more installs than most other categories, but ratings are lower, suggesting adoption without satisfaction.\"\n",
        "\n",
        "You must be able to say what happened, why it matters, and how you know.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6. Present it\n",
        "When you present, judges want:\n",
        "1. What data you got\n",
        "2. What question you asked\n",
        "3. The one plot that answers it\n",
        "4. Why that answer matters in a real setting (AI, cyber, operations, product, etc.)\n",
        "\n",
        "This is what they're scoring.\n",
        "\n",
        "You are not graded on having the fanciest model.\n",
        "You are graded on whether you can think like someone who is trusted with data.\n",
        "\"\"\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kafhyNlAqlT3",
        "outputId": "62907031-874f-4b61-a2f0-b695dd6a90a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n# Section 1 — How to Think Like an Analyst Here\nThis section is a guide. You do **not** have to follow it exactly.  \nIf you already know what you're doing, skip ahead.\n\nThe goal of this sprint is not \"make a pretty plot.\"  \nThe goal is \"find something real and explain why it matters.\"\n\nBelow is the standard workflow people use in AI, cybersecurity, data science, and engineering.\n\n---\n\n## Step 0. Understand the dataset\nBefore you write any code, answer this in plain English:\n- What does one row represent?  \n  Example: one app listing, one employee record, one purchase event, one security alert, one lift attempt.\n- What does each column approximately mean?\n  (timestamp, rating, department, severity, price, etc.)\n\nIf you can't answer what a single row means, stop here and figure that out.  \nIf you can't say what the columns are measuring, you cannot defend any conclusion later.\n\n---\n\n## Step 1. Load the data\nMost teams will be handed a CSV file.\n\nA CSV is just a table saved as plain text:\n- Comma-separated values.\n- Each line is one row.\n- First line usually has column names.\n\nIn Python, people usually load CSV into a pandas DataFrame (a table-like object you can filter, group, summarize).\n\nIn Excel / Tableau / Power BI, you'd import the CSV as a sheet or data source.\n\nMore details on loading a CSV are in a later cell.\n\n---\n\n## Step 2. Inspect it\nDo not jump straight to graphs.\nYou have to see what you're working with.\n\nQuestions to ask:\n- How many rows are there?\n- What are the column names?\n- Which columns are numbers? Which are categories? Which are timestamps?\n- Are there missing values?\n\nIf you're using pandas, people commonly call:\n- `df.head()` → first 5 rows\n- `df.info()` → column names and data types\n- `df.describe()` → summary stats\n\nIf you're not using Python, you still do the same thinking:\nscroll the first ~20 rows and write down what's normal and what's weird.\n\n---\n\n## Step 3. Pick ONE question\nYou are not trying to analyze everything at once.\nPick a single question that matters and stick to it.\n\nGood questions:\n- Which category or group is actually dominating?\n- Did something spike (traffic, attrition, cost, severity) at a certain time?\n- Are two variables clearly related (for example: price vs rating, weight vs lift total)?\n- Is there a meaningful difference between groups (for example: department A vs department B)?\n\nTie it to reality:\n- AI / model behavior\n- security / anomaly / incident\n- performance / reliability\n- business / behavior / usage\n\nIf you can't state your question in one clean sentence, you are not ready to plot.\n\n---\n\n## Step 4. Build evidence\nNow you create visual proof.\n\nThis is where plotting comes in. Typical useful plots:\n- Bar chart comparing counts across categories\n- Line plot over time\n- Scatter plot showing the relationship between two numeric variables\n- Heatmap of correlations\n\nEach of these is shown in a later cell as a template.\n\nImportant:\n- Your plot is not decoration.\n- Your plot is supposed to prove or disprove your question.\n\n---\n\n## Step 5. Write the finding in normal language\nBad: \"Here is a bar chart.\"\nGood: \"Apps in the 'Tools' category have way more installs than most other categories, but ratings are lower, suggesting adoption without satisfaction.\"\n\nYou must be able to say what happened, why it matters, and how you know.\n\n---\n\n## Step 6. Present it\nWhen you present, judges want:\n1. What data you got\n2. What question you asked\n3. The one plot that answers it\n4. Why that answer matters in a real setting (AI, cyber, operations, product, etc.)\n\nThis is what they're scoring.\n\nYou are not graded on having the fanciest model.\nYou are graded on whether you can think like someone who is trusted with data.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "display(Markdown(r\"\"\"\n",
        "# Section 2 — Team Setup and Plan\n",
        "Fill this out for yourselves. This becomes the spine of your final 3-minute presentation.\n",
        "You are allowed to change answers throughout the sprint. This is a working area.\n",
        "\n",
        "## 2.1 Team Information\n",
        "- Team number: `______`\n",
        "- Team members: `______`\n",
        "- Dataset description (high-level, not confidential name):\n",
        "  `__________________________________________`\n",
        "\n",
        "What does one row represent in your dataset?\n",
        "Example answers:\n",
        "- \"One row = one app in the Android store.\"\n",
        "- \"One row = one employee and their HR profile.\"\n",
        "- \"One row = one purchase event from a shopping weekend.\"\n",
        "- \"One row = one powerlifting meet attempt with lifter bodyweight and lift total.\"\n",
        "- \"One row = car sales numbers for a specific model in a specific region and year.\"\n",
        "\n",
        "Write your version here:\n",
        "`______________________________________________________________`\n",
        "\n",
        "If you cannot answer that, solve that now before doing anything else.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.2 Where are you working?\n",
        "Which environment are you using to analyze this data?\n",
        "\n",
        "Choose / edit:\n",
        "- Google Colab with Python\n",
        "- Jupyter / VS Code locally\n",
        "- R / RStudio\n",
        "- Excel / Power BI / Tableau\n",
        "- Custom script / other\n",
        "\n",
        "Write it here:\n",
        "`______________________________________________________________`\n",
        "\n",
        "This matters because you will be asked to walk us through your process.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.3 Core question you plan to answer\n",
        "You get one main question. Not five. One.\n",
        "\n",
        "Examples:\n",
        "- \"Which category of app actually dominates installs, and does that match rating quality?\"\n",
        "- \"Which department is losing people the fastest, and what pattern shows up with attrition?\"\n",
        "- \"What factor most strongly predicts higher lift totals at meets?\"\n",
        "- \"How did sales change over time for a specific model, and when did it jump or crash?\"\n",
        "\n",
        "Your question:\n",
        "`______________________________________________________________`\n",
        "\n",
        "Why this matters in a real AI / cyber / engineering / ops context:\n",
        "`______________________________________________________________`\n",
        "\n",
        "You will literally say this sentence to judges.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.4 What columns (fields) matter for that question?\n",
        "Pick 2–5 columns from the dataset that are relevant. For each, explain what it actually means in plain English.\n",
        "\n",
        "Example:\n",
        "- `Installs` – approximate popularity / reach of the app\n",
        "- `Rating` – user satisfaction score (1–5)\n",
        "- `Category` – app type (Tools, Game, Finance, etc.)\n",
        "- `Price` – whether the app is free or paid\n",
        "\n",
        "Your columns:\n",
        "1. `_________` means `________________________________________`\n",
        "2. `_________` means `________________________________________`\n",
        "3. `_________` means `________________________________________`\n",
        "\n",
        "If you cannot explain your columns, you cannot interpret your result.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.5 Hypothesis (this can be wrong)\n",
        "Before you analyze, take a position. Call your shot.\n",
        "\n",
        "Examples:\n",
        "- \"Paid apps have higher ratings than free apps, but way fewer installs.\"\n",
        "- \"Attrition is concentrated in high-travel roles.\"\n",
        "- \"Sales dipped in 2020, recovered in 2021.\"\n",
        "- \"Lifters with lower bodyweight but high total are clustered in specific federations.\"\n",
        "\n",
        "Your hypothesis:\n",
        "`______________________________________________________________`\n",
        "\n",
        "This is useful because:\n",
        "- If you're right, you found something.\n",
        "- If you're wrong, you found something else (the opposite).\n",
        "Either way, you have a story.\n",
        "\n",
        "---\n",
        "\n",
        "## 2.6 Planned visual\n",
        "Name one plot you think will answer your question.\n",
        "\n",
        "Examples:\n",
        "- bar chart comparing counts\n",
        "- line chart over time\n",
        "- scatter plot with color group\n",
        "- correlation heatmap\n",
        "\n",
        "Your planned visual and what you expect to prove with it:\n",
        "`______________________________________________________________`\n",
        "\n",
        "This is likely the figure you will show the judges.\n",
        "\"\"\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LbRIrA6su811",
        "outputId": "c64e7703-4ed9-4e6d-9b7a-2780b75f266f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n# Section 2 — Team Setup and Plan\nFill this out for yourselves. This becomes the spine of your final 3-minute presentation.\nYou are allowed to change answers throughout the sprint. This is a working area.\n\n## 2.1 Team Information\n- Team number: `______`\n- Team members: `______`\n- Dataset description (high-level, not confidential name):  \n  `__________________________________________`\n\nWhat does one row represent in your dataset?  \nExample answers:\n- \"One row = one app in the Android store.\"\n- \"One row = one employee and their HR profile.\"\n- \"One row = one purchase event from a shopping weekend.\"\n- \"One row = one powerlifting meet attempt with lifter bodyweight and lift total.\"\n- \"One row = car sales numbers for a specific model in a specific region and year.\"\n\nWrite your version here:  \n`______________________________________________________________`\n\nIf you cannot answer that, solve that now before doing anything else.\n\n---\n\n## 2.2 Where are you working?\nWhich environment are you using to analyze this data?\n\nChoose / edit:\n- Google Colab with Python\n- Jupyter / VS Code locally\n- R / RStudio\n- Excel / Power BI / Tableau\n- Custom script / other\n\nWrite it here:  \n`______________________________________________________________`\n\nThis matters because you will be asked to walk us through your process.\n\n---\n\n## 2.3 Core question you plan to answer\nYou get one main question. Not five. One.\n\nExamples:\n- \"Which category of app actually dominates installs, and does that match rating quality?\"\n- \"Which department is losing people the fastest, and what pattern shows up with attrition?\"\n- \"What factor most strongly predicts higher lift totals at meets?\"\n- \"How did sales change over time for a specific model, and when did it jump or crash?\"\n\nYour question:\n`______________________________________________________________`\n\nWhy this matters in a real AI / cyber / engineering / ops context:\n`______________________________________________________________`\n\nYou will literally say this sentence to judges.\n\n---\n\n## 2.4 What columns (fields) matter for that question?\nPick 2–5 columns from the dataset that are relevant. For each, explain what it actually means in plain English.\n\nExample:\n- `Installs` – approximate popularity / reach of the app\n- `Rating` – user satisfaction score (1–5)\n- `Category` – app type (Tools, Game, Finance, etc.)\n- `Price` – whether the app is free or paid\n\nYour columns:\n1. `_________` means `________________________________________`\n2. `_________` means `________________________________________`\n3. `_________` means `________________________________________`\n\nIf you cannot explain your columns, you cannot interpret your result.\n\n---\n\n## 2.5 Hypothesis (this can be wrong)\nBefore you analyze, take a position. Call your shot.\n\nExamples:\n- \"Paid apps have higher ratings than free apps, but way fewer installs.\"\n- \"Attrition is concentrated in high-travel roles.\"\n- \"Sales dipped in 2020, recovered in 2021.\"\n- \"Lifters with lower bodyweight but high total are clustered in specific federations.\"\n\nYour hypothesis:\n`______________________________________________________________`\n\nThis is useful because:\n- If you're right, you found something.\n- If you're wrong, you found something else (the opposite).\nEither way, you have a story.\n\n---\n\n## 2.6 Planned visual\nName one plot you think will answer your question.\n\nExamples:\n- bar chart comparing counts\n- line chart over time\n- scatter plot with color group\n- correlation heatmap\n\nYour planned visual and what you expect to prove with it:\n`______________________________________________________________`\n\nThis is likely the figure you will show the judges.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 3 — Loading Your CSV (Reference Only)\n",
        "\n",
        "# Most teams in this sprint will receive a `.csv` file.\n",
        "\n",
        "# A `.csv` file:\n",
        "# - Is just plain text\n",
        "# - Each row is one record\n",
        "# - Columns are separated by commas\n",
        "# - The first row usually contains column names\n",
        "\n",
        "# Below are common ways to load a CSV depending on your tool.\n",
        "# These are examples, not solutions — copy/paste them later when you’re ready.\n",
        "\n",
        "## Option A: Python (Google Colab or Jupyter)\n",
        "\n",
        "# **Step 1 – Upload your file into the runtime.**\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # you'll be prompted to choose your CSV file\n",
        "\n",
        "# After you upload, Colab will show you the filename (for example: `BlackFriday.csv` or `spotify.csv`).\n",
        "# Now you can load the data into a pandas DataFrame.\n",
        "import pandas as pd\n",
        "\n",
        "# Replace \"your_file.csv\" with the name that appeared after uploading.\n",
        "df = pd.read_csv(\"your_file.csv\")\n",
        "\n",
        "# Check the first few rows to confirm it's loaded correctly.\n",
        "df.head()\n",
        "\n",
        "# At this point, `df` is your working table (DataFrame).\n",
        "\n",
        "# - Each **column** represents one variable or feature.\n",
        "# - Each **row** represents one record.\n",
        "\n",
        "# If you see errors loading the CSV:\n",
        "# - Check the filename carefully (case-sensitive).\n",
        "# - Ensure your CSV isn’t zipped (`.zip`) or Excel (`.xlsx`) format.\n",
        "# - Try reuploading.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "jzDyVwFbwfFk",
        "outputId": "fdec4910-6f89-4721-ace1-6937b4dcbb58"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-be096256-1fcb-471e-b266-b9d0cd4a5d8f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-be096256-1fcb-471e-b266-b9d0cd4a5d8f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_file.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1622564796.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Replace \"your_file.csv\" with the name that appeared after uploading.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"your_file.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Check the first few rows to confirm it's loaded correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_file.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Option B: Python with a Direct Path\n",
        "\n",
        "# If you’ve already got the CSV in your environment, skip the upload step.\n",
        "import pandas as pd\n",
        "\n",
        "# If your file is already in your Colab environment or local folder:\n",
        "df = pd.read_csv(\"/content/your_file.csv\")  # or \"./your_file.csv\"\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "oF1KAOXoxwlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Option C: Excel / Power BI / Tableau\n",
        "\n",
        "# 1. Open your tool.\n",
        "# 2. Select **Import Data → From Text/CSV**.\n",
        "# 3. Choose the file you were given.\n",
        "# 4. Verify that columns are typed correctly:\n",
        "   # - Numbers → numeric\n",
        "   # - Dates → datetime\n",
        "   # - Text → string\n"
      ],
      "metadata": {
        "id": "1oEeXrVrx9OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Option D: R / RStudio, I do not reccomend this if you are a begginer\n",
        "\n",
        "# ```r\n",
        "# data <- read.csv(\"your_file.csv\", header = TRUE, stringsAsFactors = FALSE)\n",
        "# head(data)\n"
      ],
      "metadata": {
        "id": "shAYC6dhyC7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### After loading, stop and confirm:\n",
        "# - Do you see the columns you expected?\n",
        "# - Does each row look like what you think it represents?\n",
        "# - Are there blank columns, repeated headers, or broken rows?\n",
        "\n",
        "# If this step is wrong, every step after this will be wrong."
      ],
      "metadata": {
        "id": "cRZ_mlhfyRh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 4 — Inspecting and Cleaning Your Data\n",
        "\n",
        "# Before visualizing, always **inspect and clean** your dataset.\n",
        "# Think of this as your data “pre-flight checklist” — making sure everything looks right before analysis.\n",
        "\n",
        "# Ask yourself:\n",
        "# - What does each column represent?\n",
        "# - Are there missing values?\n",
        "# - Are any values duplicated or formatted incorrectly?\n",
        "# - What kind of data types (numeric, categorical, date/time) am I working with?\n",
        "\n",
        "# You don’t have to answer all of these now — but you should know **how** to find out.\n"
      ],
      "metadata": {
        "id": "WblQqJZZydWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the first few rows of your dataset\n",
        "df.head()\n",
        "\n",
        "# Hint: Try df.tail() to view the bottom rows.\n"
      ],
      "metadata": {
        "id": "8UsDtvuKyq6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, check the overall structure.\n",
        "# This gives you a summary of columns, data types, and how many non-null entries there are.\n",
        "\n",
        "# Think:\n",
        "# - Are any columns mostly empty?\n",
        "# - Do the column names make sense for what’s inside them?\n"
      ],
      "metadata": {
        "id": "KzNXo0lGyuOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check columns, data types, and null counts\n",
        "df.info()\n",
        "\n",
        "# Bonus: Try df.shape to see how many rows and columns your dataset has.\n"
      ],
      "metadata": {
        "id": "TFCPzyJdyz4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values and duplicates are common in real-world data.\n",
        "# Finding them early helps prevent misleading charts or broken analyses.\n"
      ],
      "metadata": {
        "id": "DnMbxTu4y1hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing (NaN) values per column\n",
        "df.isnull().sum()\n",
        "\n",
        "# Optional challenge:\n",
        "# Can you write code to show only columns with missing values?\n"
      ],
      "metadata": {
        "id": "3J8d0vHNy5R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See how many duplicate rows exist\n",
        "df.duplicated().sum()\n",
        "\n",
        "# Optional:\n",
        "# Try displaying a few duplicate rows if they exist.\n"
      ],
      "metadata": {
        "id": "Am55k53My7jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Different columns require different treatments.\n",
        "# Numerical columns can be averaged or plotted directly.\n",
        "# Categorical columns might need grouping or counting.\n",
        "\n",
        "# Before visualizing, identify **which are which**.\n"
      ],
      "metadata": {
        "id": "iJ6Equ8Ly9PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# At this point, you should have a sense of:\n",
        "# - What’s inside your data\n",
        "# - What might be broken\n",
        "# - Which columns are worth visualizing\n",
        "\n",
        "# If something looks wrong (e.g., prices listed as text, missing entries, or nonsense values),\n",
        "# try fixing just one issue and re-running your summary.\n",
        "\n",
        "# You’ll learn more by **experimenting** than following a script.\n"
      ],
      "metadata": {
        "id": "fXUVPZixzB50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 5 — Exploring Patterns and Building Visuals (Optional / Next Steps)\n",
        "\n",
        "# If you’ve cleaned your data and confirmed it looks correct — great.\n",
        "# Now it’s time to **explore, experiment, and visualize.**\n",
        "\n",
        "# You can use *any* language, library, or method you like — Python, R, Excel, Power BI, etc.\n",
        "# The following examples are purely optional and meant to help you **think** about what to try.\n"
      ],
      "metadata": {
        "id": "TobKLMlxzJK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 — Ask Questions Before Plotting\n",
        "\n",
        "Good data visualization starts with a question.  \n",
        "Ask yourself:\n",
        "\n",
        "- What story does this dataset tell?  \n",
        "- Are there trends, peaks, or correlations worth highlighting?  \n",
        "- Which variables interact in interesting ways?  \n",
        "- What could this mean for AI, cybersecurity, biology, or engineering?\n",
        "\n",
        "Write down 1–2 questions before you start plotting.  \n",
        "For example:\n",
        "- “Do users with more app reviews tend to have higher ratings?”  \n",
        "- “Are certain categories of Netflix content watched longer than others?”  \n",
        "- “Does crime frequency vary by time of day in Charlotte?”\n"
      ],
      "metadata": {
        "id": "CsCbbVs0zib5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 — Choose Visualization Tools\n",
        "\n",
        "You can use any tools. Here are some common Python options:\n",
        "\n",
        "- **matplotlib** – basic, fast plotting  \n",
        "- **seaborn** – quick, beautiful statistical visuals  \n",
        "- **plotly** – interactive, dynamic plots  \n",
        "- **pandas** – built-in plotting from dataframes  \n",
        "\n",
        "If you’re using another language or platform, that’s completely fine.  \n",
        "These examples are just for reference.\n"
      ],
      "metadata": {
        "id": "97WATov_zmC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import visualization libraries (if you need them)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# Example setup — you can comment out any you don't use\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n"
      ],
      "metadata": {
        "id": "xEwwUj1dzqsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 — Explore Relationships Between Variables\n",
        "\n",
        "Start simple:\n",
        "- Compare two numeric columns (e.g., price vs rating, downloads vs reviews)\n",
        "- Look at averages by category\n",
        "- Find outliers or unusual spikes\n"
      ],
      "metadata": {
        "id": "FTNMH1-azs6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example only — modify with your own column names\n",
        "sns.scatterplot(data=df, x=\"ColumnA\", y=\"ColumnB\")\n",
        "\n",
        "# Try swapping ColumnA and ColumnB to see different perspectives\n"
      ],
      "metadata": {
        "id": "mqxqBveYzvlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 — Summarize and Group\n",
        "\n",
        "You can also group data to look for patterns:\n",
        "- Mean sales per year\n",
        "- Average rating per category\n",
        "- Total users by country\n"
      ],
      "metadata": {
        "id": "NHT8YbH9zyco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example grouping\n",
        "df.groupby(\"CategoryColumn\")[\"NumericColumn\"].mean().sort_values(ascending=False).head(10)\n"
      ],
      "metadata": {
        "id": "kL_XcAQ5z0AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 — Think Beyond Visualization\n",
        "\n",
        "The best teams will interpret their visuals like researchers, not just show charts.  \n",
        "For inspiration:\n",
        "\n",
        "- **AI / Data Science:** Could you predict one variable using another?  \n",
        "- **Cybersecurity:** Can you detect unusual spikes or anomalies?  \n",
        "- **Biology:** Are there environmental or behavioral patterns emerging?  \n",
        "- **Engineering:** Which metrics show system performance or reliability?\n",
        "\n",
        "You don’t have to model — but showing **why** your pattern matters is powerful.\n"
      ],
      "metadata": {
        "id": "Wnv7AcUuz2tW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 — Presenting Your Findings\n",
        "\n",
        "When presenting to judges:\n",
        "1. State your dataset and your main question.  \n",
        "2. Show 1–2 clean visuals that answer it.  \n",
        "3. Explain what the patterns might mean.  \n",
        "4. If you used AI, anomaly detection, clustering, or anything advanced — describe it clearly.\n",
        "\n",
        "You can present however you like:\n",
        "- Colab notebook  \n",
        "- Slides  \n",
        "- Short verbal explanation  \n",
        "- Simple dashboard  \n"
      ],
      "metadata": {
        "id": "DaCZ3wsuz67s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Reminder\n",
        "\n",
        "This notebook is for reference — not a checklist.  \n",
        "You’re free to go in any direction that fits your dataset and story.\n",
        "\n",
        "Be creative. Think critically.  \n",
        "Visuals are just tools — the real insight comes from how *you interpret* them.\n"
      ],
      "metadata": {
        "id": "Dh_mYaPlz87j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6 — Presentation & Judging Tips\n",
        "\n",
        "You’ve reached the final stage of the sprint.  \n",
        "Now it’s time to **present your findings** — not just your charts, but your reasoning and insight.  \n",
        "\n",
        "This section will help you prepare for the 8–9 PM judging session.\n"
      ],
      "metadata": {
        "id": "Q1Rjh-Pl0PYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 — What to Present\n",
        "\n",
        "Each team will have a short presentation window to explain their analysis.\n",
        "\n",
        "You can present however you like:\n",
        "- Directly in your notebook (Colab or Jupyter)\n",
        "- Google Slides\n",
        "- Short verbal walkthrough\n",
        "- Or even a single dashboard or chart view\n",
        "\n",
        "The key is **clarity and storytelling** — not code length.\n",
        "\n",
        "When presenting:\n",
        "1. **State your dataset** (what it represents)\n",
        "2. **Share your main question or goal**\n",
        "3. **Show 1–2 visuals** that directly support your point\n",
        "4. **Explain what your data reveals**\n",
        "5. End with **a short interpretation** — why does it matter?\n"
      ],
      "metadata": {
        "id": "7iiLb9WW0P7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Presentation Flow\n",
        "\n",
        "1. “We analyzed the Spotify Tracks dataset.”\n",
        "2. “Our question was: Do older songs have lower popularity scores?”\n",
        "3. “We plotted average popularity by release year.”\n",
        "4. “We found a steady drop from 2015 onward, likely due to streaming algorithm bias.”\n",
        "5. “This could relate to how recommendation systems amplify newer content.”\n",
        "\n",
        "That’s it — simple, direct, and meaningful.\n"
      ],
      "metadata": {
        "id": "oPIFl9lZ0Syn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 — Judging Criteria\n",
        "\n",
        "Projects will be judged by HAVK officers/ future officers based on:\n",
        "\n",
        "- **Insight:** Did you find something meaningful or surprising?  \n",
        "- **Clarity:** Is your visualization readable and your story easy to follow?  \n",
        "- **Creativity:** Did you explore your data in an original way?  \n",
        "- **Technical skill:** Did you use your tools effectively (Python, AI, etc.)?  \n",
        "- **Communication:** Did you clearly explain your reasoning?\n",
        "\n",
        "Each category carries roughly equal weight — technical complexity alone does not guarantee a win.\n"
      ],
      "metadata": {
        "id": "_jLDaNrZ0Wo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 — Common Mistakes to Avoid\n",
        "\n",
        "- Too many visuals with no clear takeaway  \n",
        "- Spending all your time cleaning data without exploring it  \n",
        "- Showing models or charts you can’t explain  \n",
        "- Ignoring outliers or odd patterns instead of discussing them  \n",
        "- Focusing only on aesthetics instead of interpretation\n",
        "\n",
        "Remember: **Judges care more about your insight than your syntax.**\n"
      ],
      "metadata": {
        "id": "0lSaTGOv0Y-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 — Advanced or Thematic Tie-ins\n",
        "\n",
        "If you have time, connect your findings to your field:\n",
        "\n",
        "- **AI / Data Science:** Could your dataset train a model or inform one?  \n",
        "- **Cybersecurity:** Are there trends that could represent risk or anomaly detection?  \n",
        "- **Biology / Health:** Are there patterns in data that mirror biological behavior?  \n",
        "- **Engineering:** Could this help optimize or predict performance?\n",
        "\n",
        "These connections show real interdisciplinary thinking — exactly what top researchers and recruiters look for.\n"
      ],
      "metadata": {
        "id": "m6qBVXjN0f1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 — Closing Thoughts\n",
        "\n",
        "Whether you used AI, statistics, or just a few charts, the purpose of this sprint is **insight** — not perfection.  \n",
        "\n",
        "Show us how you think.  \n",
        "Show us what your data revealed.  \n",
        "Show us that you can connect numbers to meaning.\n",
        "\n",
        "Good luck, and thank you for participating in the HAVK Data Visualization Sprint.\n"
      ],
      "metadata": {
        "id": "I116Q2zI0i52"
      }
    }
  ]
}